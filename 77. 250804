llm 

==================================================================
챗 gpt가 정리해준 개념
1. PromptTemplate
설명:
PromptTemplate은 LLM(대규모 언어 모델)에게 전달할 **프롬프트(질문)**를 동적으로 생성하기 위한 템플릿입니다. 템플릿 내에 변수를 정의해두고, 나중에 이 변수 자리에 원하는 값을 넣어 완성된 프롬프트를 만드는 데 사용합니다.

사용법:
{변수명} 형태로 템플릿 문자열에 변수를 지정하고, PromptTemplate 객체를 생성할 때 input_variables에 해당 변수명을 리스트로 전달합니다. 이후 format() 메서드를 사용하여 변수에 값을 채워 넣어 최종 프롬프트 문자열을 얻습니다.

문법:

Python

from langchain import PromptTemplate

template = "{변수명}을(를) 활용한 문장"
prompt = PromptTemplate(template=template, input_variables=['변수명'])

# 변수에 값을 할당해 프롬프트 생성
print(prompt.format(변수명='값'))
2. ChatPromptTemplate
설명:
ChatPromptTemplate은 채팅 형식의 LLM과 상호작용하기 위해 프롬프트를 시스템 메시지, 사용자 메시지, AI 메시지 등 역할에 따라 구조화하는 데 사용됩니다. 이는 단순히 하나의 문자열로 프롬프트를 구성하는 PromptTemplate과 달리, 대화의 맥락을 명확하게 전달하는 데 효과적입니다.

사용법:
SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate 등을 사용하여 각 역할에 맞는 템플릿을 만든 후, 이들을 리스트로 묶어 ChatPromptTemplate.from_messages()에 전달합니다. 마지막으로 format_messages() 메서드를 사용하여 최종 메시지 리스트를 생성합니다.

문법:

Python

from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

# 역할별 프롬프트 템플릿 생성
system_message = SystemMessagePromptTemplate.from_template("시스템 메시지")
human_message = HumanMessagePromptTemplate.from_template("사용자 메시지: {변수}")

# 메시지 리스트로 챗 프롬프트 템플릿 생성
messages = ChatPromptTemplate.from_messages([system_message, human_message])

# 메시지 포맷팅
formatted_messages = messages.format_messages(변수='값')
3. FewShotPromptTemplate
설명:
FewShotPromptTemplate은 **소수의 예시(Few-shot examples)**를 프롬프트에 포함시켜 LLM의 응답 품질을 향상시키는 데 사용됩니다. 모델에게 원하는 응답 스타일이나 형식을 예시를 통해 학습시키는 효과가 있습니다.

사용법:
examples에 질문-답변 쌍의 예시들을 리스트 형태로 정의합니다. example_prompt로 각 예시의 형식을 지정하고, prefix로 프롬프트의 시작 부분을, suffix로 최종 질문 부분을 구성합니다.

문법:

Python

from langchain.prompts import FewShotPromptTemplate, PromptTemplate

examples = [
    {"질문": "예시 질문 1", "답변": "예시 답변 1"}
]

example_prompt = PromptTemplate(template="Q: {질문}\nA: {답변}", input_variables=['질문', '답변'])

few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="프롬프트 시작 부분",
    suffix="Q: {question}\nA:",
    input_variables=['question']
)

# 최종 프롬프트 생성 (예시에 사용되지 않은 새로운 질문 포함)
print(few_shot_prompt.format(question='새로운 질문'))
참고: suffix에는 FewShotPromptTemplate의 input_variables에 해당하는 변수만 사용해야 합니다. 모델이 답변을 생성하도록 유도하는 것이 목적이므로, answer 변수({answer})는 suffix에 포함시키지 않는 것이 일반적입니다.

4. Output Parsers
설명:
Output Parsers는 LLM이 생성한 텍스트 응답을 파이썬 객체(예: 리스트, 딕셔너리, JSON)로 변환하는 데 사용됩니다. 이를 통해 LLM의 비정형적인 텍스트 응답을 프로그래밍에 활용하기 쉬운 구조적인 데이터로 가공할 수 있습니다.

사용법:
먼저 원하는 파서를 선택하고(CommaSeparatedListOutputParser, JsonOutputParser 등), get_format_instructions() 메서드를 통해 모델에게 응답 형식을 알려줄 지시문을 받습니다. 이 지시문을 프롬프트에 포함시켜 모델이 특정 형식으로 답변하도록 유도합니다. 마지막으로, 모델의 응답을 parse() 메서드에 전달하여 파싱합니다.

문법:

Python

from langchain.output_parsers import CommaSeparatedListOutputParser

output_parser = CommaSeparatedListOutputParser()
format_instructions = output_parser.get_format_instructions()

# 프롬프트에 형식 지시문 포함
prompt_template = PromptTemplate(
    template="... \n형식지정: {format_instructions}",
    ...
)

# ... 모델 응답을 받은 후
response_content = "foo, bar, baz"
parsed_list = output_parser.parse(response_content)
5. HuggingFaceEndpoint
설명:
HuggingFaceEndpoint는 로컬에 모델을 설치하지 않고, Hugging Face Hub에 호스팅된 다양한 LLM을 원격으로 호출하여 사용할 수 있게 해주는 클래스입니다. 이를 통해 복잡한 설정 없이 다양한 모델을 쉽게 테스트해볼 수 있습니다.

사용법:
repo_id에 사용하려는 모델의 허브 ID를 지정하고, task에 모델의 사용 목적(예: text-generation)을 지정하여 HuggingFaceEndpoint 객체를 생성합니다. huggingfacehub_api_token을 제공하면 비공개 모델이나 유료 모델도 사용할 수 있습니다.

문법:

Python

from langchain_huggingface import HuggingFaceEndpoint

endpoint = HuggingFaceEndpoint(
    repo_id="모델_레포지토리_ID",
    task='text-generation',
    huggingfacehub_api_token="YOUR_TOKEN" # 필요시
)

# ChatHuggingFace와 결합하여 사용
from langchain_huggingface import ChatHuggingFace
model = ChatHuggingFace(llm=endpoint)
참고: HuggingFaceEndpoint를 사용할 때 ValueError가 발생하면 huggingfacehub_api_token을 올바르게 설정했는지 확인해야 합니다.

==============================================================================================
수업 내용 코드!! 

from dotenv import load_dotenv

load_dotenv()
===============================================
%pip install langchain
===============================================
from langchain import PromptTemplate

template = "{product}를 홍보하기 위한 재미있고, 새로운 광고문구를 작성해 주세요."

prompt = PromptTemplate(
    template=template,
    input_variables=['product']
)

print(prompt.format(product='카메라'))
print(prompt.format(product='커피'))
---------------------------
카메라를 홍보하기 위한 재미있고, 새로운 광고문구를 작성해 주세요.
커피를 홍보하기 위한 재미있고, 새로운 광고문구를 작성해 주세요.
===============================================
from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate

system_message = SystemMessagePromptTemplate.from_template("당신은 도움을 주는 챗봇 입니다.")
print(system_message)

human_message = HumanMessagePromptTemplate.from_template("질문: {question}")
print(human_message)

messages = ChatPromptTemplate.from_messages([system_message, human_message])
print(messages)
print()

prompt = messages.format_messages(question="AI가 무엇인가요?")
print(prompt)
---------------------------------------------
prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 도움을 주는 챗봇 입니다.') additional_kwargs={}
prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='질문: {question}') additional_kwargs={}
input_variables=['question'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 도움을 주는 챗봇 입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='질문: {question}'), additional_kwargs={})]

[SystemMessage(content='당신은 도움을 주는 챗봇 입니다.', additional_kwargs={}, response_metadata={}), HumanMessage(content='질문: AI가 무엇인가요?', additional_kwargs={}, response_metadata={})]
===============================================
from langchain.prompts import FewShotPromptTemplate

examples = [
    {"question": "2+2는 무엇인가요?", "answer": "4입니다"}, 
    {"question": "3+7는 무엇인가요?", "answer": "10입니다"}    
]

example_prompt = PromptTemplate(
    template="Q: {question} \nA: {answer}", 
    input_variables=['question', 'answer']
)

few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="다음 계산문제를 해결하세요.",
    suffix="Q: {question} \nA: ",
    # suffix="Q: {question} \nA: {answer}", 
    input_variables=['question']
)

prompt = few_shot_prompt.format(question="25+84는 무엇인가요?")
print(prompt)
-----------------------------
다음 계산문제를 해결하세요.

Q: 2+2는 무엇인가요? 
A: 4입니다

Q: 3+7는 무엇인가요? 
A: 10입니다

Q: 25+84는 무엇인가요? 
A: 

*** # suffix="Q: {question} \nA: {answer}" 쓰면 에러가 남 왜일까? 
❌ 여기에서 'answer'도 필요하다고 되어 있음 > FewShotPromptTemplate의 suffix에 {answer} 변수가 포함되어 있는데
format() 호출 시에는 question="..."만 전달하고 answer를 전달하지 않았기 때문에 발생 
>> few_shot_prompt.format(question="25+84는 무엇인가요?")만 호출했기 때문에 answer는 아직 없어서 오류가 납니다. 
해결 방법은 suffix에서 {answer}를 제거하는 것입니다. 
모델이 답변을 생성하도록 유도하는 것이 목적이므로 보통 suffix는 질문까지만 포함한다. 

===============================================
%pip install langchain langchain-openai openai
===============================================
from langchain.output_parsers import CommaSeparatedListOutputParser
from langchain_openai import ChatOpenAI

output_parser = CommaSeparatedListOutputParser() #최종적으로 파이썬 리스트형태로 반환 
format_instructions = output_parser.get_format_instructions()
print(format_instructions)

prompt_template = PromptTemplate(
    template="{subject} 10개의 팀을 보여주세요. \n형식지정: {format_instructions}", 
    input_variables=['subject'],
    partial_variables={'format_instructions':format_instructions} #고정적(기본적으로 포함)으로 설정될 변수
)

query = "한국의 아구팀"
prompt = prompt_template.format(subject=query)
print(prompt)

model = ChatOpenAI(
    model="gpt-4o",
    temperature=1,
    max_tokens=2048
)

response = model.invoke(prompt)
print(response.content)
print(type(response.content))

#파서를 통해 형변환 (str -> list)
parsed_response = output_parser.parse(response.content)
print(parsed_response)
print(type(parsed_response))
---------------------------------
Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`
한국의 아구팀 10개의 팀을 보여주세요. 
형식지정: Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`
기아 타이거즈, 삼성 라이온즈, LG 트윈스, 두산 베어스, 롯데 자이언츠, SSG 랜더스, 한화 이글스, NC 다이노스, KT 위즈, 키움 히어로즈
<class 'str'>
['기아 타이거즈', '삼성 라이온즈', 'LG 트윈스', '두산 베어스', '롯데 자이언츠', 'SSG 랜더스', '한화 이글스', 'NC 다이노스', 'KT 위즈', '키움 히어로즈']
<class 'list'>
===============================================
#chatopenai 
from langchain_openai import OpenAI

model1=ChatOpenAI(
    model_name='gpt-4o',
    temperature=0, 
    max_tokens=2048
)
model1.invoke('은정이는 강아지를 키우고 있습니다. 은정이가 키우고 있는 동물은 무엇인가요?')
-------------------------
AIMessage(content='은정이가 키우고 있는 동물은 강아지입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 32, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-C0goUcRteeHOrDjIeHgjt5ivxpGCb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--2436da4a-03df-43e5-8129-3a383f5f7b5a-0', usage_metadata={'input_tokens': 32, 'output_tokens': 15, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})

===============================================
%pip install langchain_huggingface
===============================================
#huggingface 

from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace

endpoint = HuggingFaceEndpoint(
    repo_id="mistralai/Mixtral-8x7B-Instruct-v0.1",
    task='text-generation', 
    max_new_tokens=4096   
)

model2 = ChatHuggingFace(
    llm=endpoint,
    verbose=True
)

model2.invoke('은정이는 강이지를 키우고 있습니다. 은정이가 키우는 동물은 무엇인가요?')
---------------------
AIMessage(content=" 강이(Kangi)는  Виetedespride's 영문 TRANSLIT (Transfer Language) 아이디로, 한국어의 『강아zi』(小아zi) 나elsonnene's 아이디인 ‘은정이(SeungEe)’가 사용하는 별칭(Nickname)입니다. \n\n따라서, ‘은정이(SeungEe)’는 아직도 강이(Kangi)가 키우는 동물에 대해서 설명하지 않았기 때문에,  Answer the Question 는 현재 이 정보를 제공할 수 없습니다.", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 178, 'prompt_tokens': 50, 'total_tokens': 228}, 'model_name': 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--8a357a3d-4b98-4841-88ef-a0eaf1e85ea1-0', usage_metadata={'input_tokens': 50, 'output_tokens': 178, 'total_tokens': 228})

** 허깅페이스 repo_id Bllossom/llama-3.2-Korean-Bllossom-3B로 실행 > 에러남 
주소 바꿔서 다시실행 (한글 깨짐 이슈 있음)
===============================================
from langchain.model_laboratory import ModelLaboratory

model_lab = ModelLaboratory.from_llms([model1, model2])

model_lab.compare('대한민국의 여름은 몇월부터 몇월인가요?')
----------------------------
Input:
대한민국의 여름은 몇월부터 몇월인가요?

client=<openai.resources.chat.completions.completions.Completions object at 0x00000141A3B156D0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000141A30BC290> root_client=<openai.OpenAI object at 0x00000141A3A6F590> root_async_client=<openai.AsyncOpenAI object at 0x00000141A3B15340> model_name='gpt-4o' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********') max_tokens=2048
대한민국의 여름은 일반적으로 6월부터 8월까지로 여겨집니다. 이 시기는 기온이 높고 습도가 높은 특징을 보이며, 장마철도 포함되어 있습니다. 

llm=HuggingFaceEndpoint(repo_id='mistralai/Mixtral-8x7B-Instruct-v0.1', huggingfacehub_api_token='huggingfacehub_api_token', max_new_tokens=4096, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='mistralai/Mixtral-8x7B-Instruct-v0.1', client=<InferenceClient(model='mistralai/Mixtral-8x7B-Instruct-v0.1', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mixtral-8x7B-Instruct-v0.1', timeout=120)>, task='text-generation') model_id='mistralai/Mixtral-8x7B-Instruct-v0.1' model_kwargs={}
 대한민국의 여름은 평년에는  Juche �lishedCalendar6 Month(윤rical 6 Month, Normally June) 부터 Juche �lishedCalendar9 Month(윤rical 9 Month, Normally August)까지이오니다. 그러니까, 일반적으로 온dog-hot summer starts from June and ends in August in Korea. 물론, 요즘은 날씨가 덥고 기후가 변하고 있어서 예외적으로 메디트레이��ыNan(Mediteranean) 같은 날씨가 나오는 경우도 있구나. 그래도, 대체적으로 summer time in Korea is from June to August.

===============================================
