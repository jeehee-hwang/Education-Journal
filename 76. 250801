개인 사정으로 수업 빠짐 

voicebot  만듬 

=============================================
app.py

import streamlit as st
from audiorecorder import audiorecorder
from streamlit_chat import message as msg
import openai_api

def main():
    st.set_page_config(
        page_title='😎Voice Chatbot😎',
        page_icon="💡",
        layout='wide'
    )
    st.header('Voice Chatbot')
    st.markdown('---')

    with st.expander('Voice Chatbot 프로그램을 사용하는 방법', expanded=False):
        st.write(
            """
            1. 녹음하기 버튼을 눌러 질문을 녹음합니다.
            2. 녹음이 완료되면 자동으로 Whisper 모델을 이용해 음성을 텍스트로 변환후 LLM에 질의합니다.
            3. LLM이 응답을 다시 TTS모델을 사용해 음성으로 변환하고 이를 사용자에게 응답합니다.
            4. LLM은 OpenAI사의 GPT모델을 사용합니다.
            5. 모든 질문/답변은 텍스트로도 제공합니다.
            """
        )

    system_instruction = '당신은 친절한 챗봇입니다.'

    # session state 초기화
    # - chats: 웹페이지 시각화용 대화내역
    # - messages : LLM 질의/웹페이지 시각화를 위한 대화내역
    # - check_reset: 사이드바 초기화 버튼 활성화용

    if 'messages' not in st.session_state:
        st.session_state['messages'] = [
            {'role':'system', 'content': system_instruction}
        ]

    if 'check_reset' not in st.session_state:
        st.session_state['check_reset'] = False

    with st.sidebar:
        model = st.radio(label='GPT 모델', options=['gpt-3.5-turbo', 'gpt-4-turbo', 'gpt-4o'], index=2)
        print(model)

        if st.button(label='초기화'):
            st.session_state['messages'] = [
                {'role':'system', 'content': system_instruction}
            ]
            st.session_state['check_reset'] = True

    col1, col2 = st.columns(2) # 화면을 두개의 열로 나눕니다.
    with col1:
        st.subheader('녹음하기')

        audio = audiorecorder()
        
        if (audio.duration_seconds > 0) and (st.session_state['check_reset'] == False):
            # 화면상의 재생기능
            st.audio(audio.export().read())
            query = openai_api.stt(audio)
            print('Q:', query)
            # 변환된 텍스트를 사용자의 메시지로 session_state에 추가해서 대화 기록을 남긴다.
            st.session_state['messages'].append({'role':'user', 'content':query})
            response = openai_api.ask_gpt(st.session_state['messages'], model)
            print('A:', response)
            # GPT 답변도 session_state에 추가해준다.
            st.session_state['messages'].append({"role":"assistant", 'content': response})
            # GPT 답변을 음성으로 변환
            audio_tag = openai_api.tts(response)
            st.html(audio_tag) # 시각화되지 않고, 자동으로 재생(즉시 재생 가능한 html 태그)

    with col2:
        st.subheader('질문/답변')
        if(audio.duration_seconds >0 ) and (st.session_state['check_reset'] == False): 
            for i, message in enumerate(st.session_state['messages']):
                role = message['role']
                content = message['content']
                if role == 'user':
                    msg(content, is_user=True, key=str(i), avatar_style="big-smile")
                elif role == 'assistant':
                    msg(content, is_user=False, key=str(i), avatar_style="croodles-neutral")
        else:
            st.session_state['check_reset'] = False


if __name__ == '__main__':
    main()

==================================
openai_api.py

import base64
from dotenv import load_dotenv
from openai import OpenAI
import os

load_dotenv()

client = OpenAI()

def stt(audio):
    filename = 'temp.mp3' # 오디오 데이터를 저장할 임시 파일 이름
    audio.export(filename, format='mp3')

    with open(filename, 'rb') as f:
        transcription = client.audio.transcriptions.create(
            model="whisper-1",
            file=f
        )
    os.remove(filename) # 임시파일 삭제
    return transcription.text

def ask_gpt(messages, model):
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=1,
        max_tokens=4096,
        top_p=1
    )
    return response.choices[0].message.content

def tts(text):
    filename = 'output.mp3'
    with client.audio.speech.with_streaming_response.create(
        model='tts-1',
        voice='fable',
        input=text
    ) as response:
        response.stream_to_file(filename)

    # Base64 인코딩 : 음성 파일 데이터 자체를 아주 긴 텍스트(base64)로 변환한다.
    with open(filename, 'rb') as f:
        data = f.read()
        b64_encoded = base64.b64encode(data).decode()
        audio_tag =f"""
        <audio autoplay="true"
            <source src="data:audio/mp3;base64,{b64_encoded}" type='audio/mp3'/>
        </audio>
        """
    os.remove(filename) # 원본 삭제

    return audio_tag    # 파일이 아닌, 오디오 정보가 담긴 HTML 코드 조각을 반환

======================================================
session_state.py

import streamlit as st

st.title("Session State")

st.subheader("전역변수 Count")

count = 0

if st.button('Increment - 전역변수 Count'):
    count += 1

st.write(f'전역변수 Count: {count}')

#---------------------------------------------#

st.subheader("st.session_state Count")

# session_state 초기화
if 'count' not in st.session_state:
    st.session_state['count'] = 0

if st.button('Increment - st.session_state Count'):
    st.session_state['count'] += 1

st.write(f'전역변수 Count: {st.session_state['count']}')


